\chapter{Application}\label{D:application}

This chapter aims to develop the tasks introduced for the application layer in this project Gantt (see figure \ref{F:tpgp}).

\section{REST API}

As previously said, it's required to develop an API ready to be used over cloud environments in order to ease creating specific and new applications over the LMS framework, and thus to demonstrate the viability of this project prototype.

Nowadays, the common and widely used format for cloud services intercommunication is JSON, as it is also used for the TCP socket API of the LMS framework. Therefore, this API middleware is going to follow such premise.

A suitable technology to work with JSON formatted messages which is widely known for its good performance is Node.js (--REFERENCE--). Node.js is an open source, cross-platform runtime environment for server-side and networking applications. It provides an event-driven architecture and a non-blocking I/O API that optimizes application's throughput and scalability. This technology is commonly used for real-time web applications. 

Moreover, working with Node.js means avoiding serialization of the JSON messages by increasing services intercommunication performance (i.e.: less computational cost and less processing time). 

Then, the common Noje.js framework for developing web applications and REST APIs is Express.js (--REFERENCE--). It's the de facto standard server framework for Node.js. So, the middleware to develop is going to use its routing system, which refers to the definition of end points (URIs with HTTP request methods like GET, POST, PUT and DELETE) to an application and how it responds to client requests.

Following is the software structure proposal for developing the HTTP RESTfull API middleware, which is responsible to translate to the TCP socket API of the LMS framework:

\begin{figure}[!htb]
\begin{center}
\includegraphics[width=1\textwidth]{./images/RESTAPI.png}
\caption{RESTfull API middleware architecture}
\label{F:restAPI}
\end{center}
\end{figure}

\begin{itemize}
\item HTTP REST API layer \hfill

This layer handles HTTP queries from external applications. It implements specific routes to handle specific HTTP queries. First implementation will not implement multiple LMS management but single. As shown in figure \ref{F:restAPI}.

\item Interface layer \hfill

This layer handles the body messages from the previous layer's HTTP queries and manipulates them in order to create an as much generic as possible API by adapting the messages to be sent through following layer.

\item TCP socket layer \hfill

This layer is the responsible to send and receive JSON formatted TCP socket messages between the LMS instance targeted.
\end{itemize}

As introduced in section {SOA:LMSframework} and deeply explained in APPENDIX \ref{ANX:lmsarchfull}, there are two different management layers: the generic and the filter specific. So, by following this organization, the proposed API's structure is as shown next:

\begin{itemize}
\item Generic management queries
\begin{itemize}
\item Connect \hfill

Checks if an existing instance of LMS is running and sets the LMS port and LMS host to work with.
\begin{quote}
\begin{verbatim}
POST http://<host>:<port>/api/connect
JSON    {
            "port":<lms-port>,
            "host":"<lms-host>"
        }
\end{verbatim}
\end{quote}
\item Disconnect \hfill

Resets the running LMS instance and sets \verb|lms-port| and \verb|lms-host| to null in order to connect again to the same or any another LMS instance running.
\begin{quote}
\begin{verbatim}
GET http://<host>:<port>/api/disconnect
\end{verbatim}
\end{quote}
\item State \hfill

Gets the state object of the current LMS instance connected to (JSON object with the configured filters and paths).
\begin{quote}
\begin{verbatim}
GET http://<host>:<port>/api/state
\end{verbatim}
\end{quote}
\item Create a filter \hfill

Creates a filter (current types are: receiver, transmitter, demuxer, dasher, audioDecoder, audioEncoder, videoDecoder, videoResampler, videoEncoder, audioMixer, videoMixer) with an unique identifier.
\begin{quote}
\begin{verbatim}
POST http://<host>:<port>/api/createFilter
JSON    {
            "id": filterID,
            "type": "type"
        }
\end{verbatim}
\end{quote}        
\item Create a path of filters \hfill

Create a path of filters. A path can be a master path or an slave one, as shown:

\begin{itemize}

\item Master path
\begin{quote}
\begin{verbatim}
POST http://<host>:<port>/api/createPath
JSON    { 
            'id' : pathId, 
            'orgFilterId' : orgFilterId, 
            'dstFilterId' : dstFilterId, 
            'orgWriterId' : orgWriterId, 
            'dstReaderId' : dstReaderId, 
            'midFiltersIds' : [filterID1, filterID2,...] 
        }
\end{verbatim}
\end{quote}
\item Slave path of previous master path
\begin{quote}
\begin{verbatim}
POST http://<host>:<port>/api/createPath
JSON    { 
            'id' : pathId, 
            'orgFilterId' : filterID1, 
            'dstFilterId' : dstFilterId2, 
            'orgWriterId' : -1, 
            'dstReaderId' : dstReaderId2, 
            'midFiltersIds' : [filterID3, filterID4,...] 
        }  
\end{verbatim}
\end{quote}
\end{itemize}
\end{itemize}
\item Specific filter management queries        
\begin{itemize}
\item Configure an existing filter \hfill

Configures an existing filter (each filter has its own actions defined, check APPENDIX \ref{ANX:lmsarchfull})
\begin{quote}
\begin{verbatim}
PUT http://<host>:<port>/api/filter/:filterID
JSON    [
            {
                "action":"the action",
                    "params":{
                        "param1":param1,
                        "param2":"param2",
                        "param3":"param3",
                        "param4":true
                }
            }
        ]
\end{verbatim}
\end{quote}

\end{itemize}
\end{itemize}

So, with previous API proposal, the whole LMS's TCP socket API becomes simplified.

Moreover, specific responses format to client has been proposed as shown:

\begin{itemize}
\item Success messages \hfill

It may be a string, bool, array or another object, depending on the request method

\begin{quote}
\begin{verbatim}
JSON    {
            "message": { the incoming message }
        }
\end{verbatim}
\end{quote}        
\item Error messages \hfill

\begin{quote}
\begin{verbatim}
JSON    {
            "error": "the error message"
        }
\end{verbatim}
\end{quote}
\end{itemize}

To point out that this API is not implementing persistence because the state (managed through 'State' method) is given by the LMS instance itself. The unique sign of persistence is regarding the LMS host and port which the middleware is connected to (managed through 'Connect' and 'Disconnect' methods). Higher levels of persistence should be implemented by external applications which implies specific scenarios and requirements (i.e.: specific persistence).

Finally, in order to know how this structure and the overall middleware is implemented check APPENDIX \ref{ANX:sourceCodes} to see the code. Next chapters will demonstrate it too.

\section{Network metrics}

Network metrics could be treated as external metrics. This is because these metrics are specially dependent from sources which transmit to LMS, the receivers from LMS and the state of the network itself. Obviously, the performance of the LMS affect to the metrics gathered too, but it is intended to be minimized, at least in a gathering and presentation of the metrics' point of view.

\subsection{Input network metrics}

As said in subsection \ref{B:appLayerCH2}, input network metrics are going to be implemented by carrying out methods re-implementations given by the Live555 library, which is the library implemented for managing network streams.  

\begin{figure}[!htb]
\begin{center}
\includegraphics[width=0.45\textwidth]{./images/SourceManager.png}
\caption{Input network metrics structure}
\label{F:inms}
\end{center}
\end{figure}

By following the LiveMediaStreamer architecture structure, input network implementations are going to be implemented inside the 'liveMediaInput' structure. Concretely, a new class is implemented, called 'SCSSubsessionStats'. This class is managed by the 'StreamCleanState' class which is a class related to each stream 'Session' class managed by the 'SourceManager' class. This last class is a 'HeadFilter' class. Figure \ref{F:inms} shows the inter-class structure.

By initializing new RTP or RTSPClient sessions (i.e.: network inputs), a group of subessions is associated per each stream (i.e.: an RTP session has one subsession associated and the RTSPClient session has as many subsessions as accepted from the SDP that defines different RTP sessions).

When a new subsession is set, then, automatically, a new RTPReceiverStats class is initialized. This Live555 object implements RTCP stats measurement which are only required to be treated outside. This is done at SCSSubsessionState, which creates a new schedule to periodically measure and save current state (a default granularity of 1 second is set).

The implemented method that is called periodically is as shown:

\begin{quote}
\begin{verbatim}
void SCSSubsessionStats::
			periodicStatMeasurement(struct timeval const& timeNow) 
{
    unsigned secsDiff = timeNow.tv_sec - measurementEndTime.tv_sec;
    int usecsDiff = timeNow.tv_usec - measurementEndTime.tv_usec;
    double timeDiff = secsDiff + usecsDiff/1000000.0;
    measurementEndTime = timeNow;

    RTPReceptionStatsDB::Iterator statsIter(fSource->receptionStatsDB());

    RTPReceptionStats* stats = statsIter.next(True);
    if (stats != NULL) {
        double kBytesTotalNow = stats->totNumKBytesReceived();
        double kBytesDeltaNow = kBytesTotalNow - kBytesTotal;
        kBytesTotal = kBytesTotalNow;

        double kbpsNow = timeDiff == 0.0 ? 0.0 : 8*kBytesDeltaNow/timeDiff;
        if (kbpsNow < 0.0) kbpsNow = 0.0; // in case of roundoff error
        if (kbpsNow < kbitsPerSecondMin) kbitsPerSecondMin = kbpsNow;
        if (kbpsNow > kbitsPerSecondMax) kbitsPerSecondMax = kbpsNow;

        unsigned totReceivedNow = stats->totNumPacketsReceived();
        unsigned totExpectedNow = stats->totNumPacketsExpected();
        unsigned deltaReceivedNow = totReceivedNow - totNumPacketsReceived;
        unsigned deltaExpectedNow = totExpectedNow - totNumPacketsExpected;
        totNumPacketsReceived = totReceivedNow;
        totNumPacketsExpected = totExpectedNow;

        double lossFractionNow = deltaExpectedNow == 0 ? 0.0 : 1.0 -
        							 deltaReceivedNow/(double)deltaExpectedNow;

        if (lossFractionNow < packetLossFractionMin) {
            packetLossFractionMin = lossFractionNow;
        }
        if (lossFractionNow > packetLossFractionMax) {
            packetLossFractionMax = lossFractionNow;
        }

        minInterPacketGapUS = stats->minInterPacketGapUS();
        maxInterPacketGapUS = stats->maxInterPacketGapUS();
        totalGaps = stats->totalInterPacketGaps();
        jitter = stats->jitter();
    }
}
\end{verbatim}
\end{quote} 

Previous algorithm prepares the metrics that are going to be presented through the state when a new state query is received. In the code is shown how metrics from Live555 library are obtained. For a more detailed insight of the overall implementation see APPENDIX \ref{ANX:sourceCodes}. Next are listed the metrics that are presented per each new state query:

\begin{itemize}
\item Bitrate: maximum, minimum and average in kbps.
\item Packet loss percentage: maximum, minimum and average.
\item Inter-packet gap: maximum, minimum and average in milliseconds.
\item Jitter: maximum, minimum and current inter-packet gap variation in microseconds.
\end{itemize}

Another metric that might be of interest is the time delay from the stream source but it is discarded due to not being offered from Live555 library. Moreover, to be implemented at SCSSubsessionStats class level has been discarded due to is computational cost and complexity to develop such requirement. But, to point out that it is not a high priority required metric due to take into account that this issue is out of LMS performance scope and if there are network performance problems they can be detected through other metrics already gathered.


\subsection{Output network metrics}

As done in previous section, output network metrics are going to be implemented by carrying out methods re-implementations given by the Live555 library, which is the library implemented for managing network streams.

This implementation has been much more difficult to be achieved due to not having control of the RTPSink class of the Live555 library in the sense that when it is created neither deleted. Previous developments before the final version where based over the RTPSink re-implementation already done per each OnDemandServerMediaSubsession (Live555 library class), which is also re-implemented by QueueServerMediaSubsession (LMS framework class). But the implementation was still losing the specific RTPSink instance of specific subsession.

In order to go on the development an e-mail was sent to the Live555 developers mailing list, concretely to the head developer of the library, thanks to his replay the solution was clarified (see APPENDIX \ref{ANX:emailRoss}).

So, the best option, as suggested by Ross, was to re-implement the RTCPInstance class per each inheriting class of the OnDemandServerMediaSubsession class, concretely the inheriting classes of the QueueServerMediaSubsession class. 

\begin{figure}[!htb]
\begin{center}
\includegraphics[width=0.45\textwidth]{./images/SinkManager.png}
\caption{Output network metrics structure}
\label{F:onms}
\end{center}
\end{figure}

In figure \ref{F:onms} is shown the relationship between the SinkManager class with both possible types of connections (i.e.: RTP or RTSP). Each Connetion object has a map of objects of the re-implemented RTCPInstance class, called ConnRTCPInstance (i.e.: RTCP instances per each connection). Then, each time a new specific RTP connection or any QueueServerMediaSubsession (RTSP connection, from RTSP server) is created, a ConnRTCPInstance is associated in order to start gathering the statistics offered from Live555 library. This is shown in next piece of code by showing the method which is periodically called (default periodicity value is set to 1 second), as done for the input network metrics.

\begin{quote}
\begin{verbatim}
void ConnRTCPInstance::
			periodicStatMeasurement(struct timeval const& timeNow) 
{
    unsigned currentNumBytes;
    double currentElapsedTime;

    RTPTransmissionStatsDB::Iterator 
    							statsIter(fSink->transmissionStatsDB());

    fSink->getTotalBitrate(currentNumBytes, currentElapsedTime);

    avgBitrate = currentElapsedTime == 0 ? 0.0 :
    					((8*currentNumBytes/currentElapsedTime)/1000.0);
    if(minBitrate > avgBitrate) minBitrate = avgBitrate;
    if(maxBitrate < avgBitrate) maxBitrate = avgBitrate;

    RTPTransmissionStats* stats;
    while((stats = statsIter.next()) != NULL){
        SSRC = stats->SSRC();
        packetLossRatio = stats->packetLossRatio();
        if(minPacketLossRatio > packetLossRatio) 
        							minPacketLossRatio = packetLossRatio;
        if(maxPacketLossRatio < packetLossRatio) 
        							maxPacketLossRatio = packetLossRatio;        
        
        roundTripDelay = stats->roundTripDelay();
        if(minRoundTripDelay > roundTripDelay) 
        							minRoundTripDelay = roundTripDelay;
        if(maxRoundTripDelay < roundTripDelay) 
        							maxRoundTripDelay = roundTripDelay;

        jitter = stats->jitter();
        if(minJitter > jitter) minJitter = jitter;
        if(maxJitter < jitter) maxJitter = jitter;
    }
}
\end{verbatim}
\end{quote} 

In this case, the delay metric is offered from the Live555 library, which is gathered.

Finally, presented metrics are as shown next:

\begin{itemize}
\item Bitrate: maximum, minimum and average in kbps.
\item Packet loss percentage (ratio): maximum, minimum and current.
\item Round trip delay: maximum, minimum and current in milliseconds.
\item Jitter: maximum, minimum and current inter-packet gap variation in microseconds.
\end{itemize}

\section{Pipeline metrics}

Following metrics are intended to be exposed as the internal metrics. It's important to point out that the measurements behind are really responsive to the overall performance of the LMS framework. Therefore, to achieve the minimum computational cost is a must. Then, as introduced in subsection \ref{B:appLayerCH2}, the optimal solutions to gather and measure such metrics is as shown next. But, first of all, let's pick up the example figure of a pipeline from APPENDIX \ref{ANX:lmsarchfull} in order to showcase the internal pipeline structure of the LMS framework. This will help understanding next two implementations.

\begin{figure}[!htb]
\begin{center}
\includegraphics[width=0.9\textwidth]{./images/LMSpipelineBasicOne.png}
\caption{LMS framwork's internal pipeline structure example}
\label{F:lmsps}
\end{center}
\end{figure}

To point out that in figure \ref{F:lmsps} the arrows are the queues which interconnect each filter's writer with another filter's reader. Writers and readers are subclasses of the IOInterface class.

\subsection{Frame delay metrics}

First of all, to emphasize that this metric is related to the time which a frame takes to be processed from an origin point to an end point by an unique given path, and this is measured thanks to a given and required time: the frame's timestamps. 

The delay time isn't considered to be measured per each filter in order to not affect the overall performance but each measured time involves an origin filter which resets the timestamps (i.e.: this is done by default by the Live555 library). These origins are the Head (i.e.: receiver filter) and OneToMany (i.e.: audio and video mixer filters) filters. These filters reset the frames' timestamps in order to reach and control an internal synchronization, and this is due to the fact that these filters have many outputs (i.e.: writers) or many inputs (i.e.: readers) and they require synchronizing their outputs in order to assure one point of time control inside the LMS framework, whatever the scenario configured.

So, in order to measure the delay it is important to note that a pipeline isn't composed by an unique path (see APPENDIX \ref{ANX:lmsarchfull} for clarifications) but multiple paths. This fact implies that it's not possible to measure an unique overall delay time per frame which goes over the pipeline (indeed there is a special case, and this is when there is a unique path that defines the pipeline itself, an unique path), or at least it's not suitable for performance issues, but it can be done for external applications which know the scenario configuration and gathers such metrics. Therefore, this is solved by splitting the measurements by paths. And this is an optimal measurement: the delay is given by the differential time measured by the last reader of the path (i.e.: the reader of the destination filter).

Next is shown the method which implements the measurement inside the Reader class:

\begin{quote}
\begin{verbatim}
void Reader::measureDelay()
{
    if(lastTs.count() < 0){
        lastTs = frame->getPresentationTime();
    }

    if (lastTs == frame->getPresentationTime()) {
        return;
    }

    timeCounter += frame->getPresentationTime() - lastTs;
    lastTs = frame->getPresentationTime();

    if(timeCounter >= windowDelay){
        avgDelay = delay / frameCounter;
        timeCounter = std::chrono::microseconds(0);
        delay = std::chrono::microseconds(0);
        frameCounter = 0;
    }
    
    delay += std::chrono::duration_cast<std::chrono::
    						microseconds>(std::chrono::system_clock::now() 
    											- frame->getOriginTime());
    frameCounter++;
}
\end{verbatim}
\end{quote} 

So, what is done is to measure the average delay of a frame by a given window time (default is configured to 1 second). And this means measuring from the origin time, which, as said, is set by the origin filters (i.e.: the beginning of a path, starting from a writer), to each reader of the path (i.e.: each filter). But, the delay time presented is from the beginning (i.e.: initial writer of the path) until the last reader. See figure \ref{F:lmsps} where different path's examples are illustrated.

\subsection{Frame losses metrics}

This metric is following the same criteria as the previous one. This means that this is solved by measuring at the same point, so it's done when flushing frames at the reader side (as introduced in section \ref{B:appLayerCH2}. In order to reach minimal computational cost, the method implemented is as shown next:

\begin{quote}
\begin{verbatim}
void Reader::measureLosses()
{
    if(frame->getSequenceNumber() > (lastSeqNum + 1)){
        lostFrames += (frame->getSequenceNumber() - (lastSeqNum + 1));
    }
    lastSeqNum = frame->getSequenceNumber();
}
\end{verbatim}
\end{quote} 

Thanks to the sequence number parameter that each frame has it is quite easy to get the frame losses ratio, as exposed by previous method's implemented.

At the end, the metric presented in this case is the total frames lost and the total frames by a given path, per each filter. So, what is presented is the frame loss ratio per path.

Finally, as said, Pipeline metrics are measured when flushing frames, this means when a reader is able to remove a frame from its belonging queue. Next piece of code from the Reader class is showing how it is finally done for both frame measurements:

\begin{quote}
\begin{verbatim}
int Reader::removeFrame(int fId)
{
    std::lock_guard<std::mutex> guard(lck);

    if (pending != 0 && requests.count(fId) > 0 && requests[fId]){
        pending--;
        requests[fId] = false;
    }
    
    if (pending == 0){

        measureDelay();
        measureLosses();

        frame = NULL;
        requests.clear();
        return queue->removeFrame();
    } else {
        return -1;
    }
}
\end{verbatim}
\end{quote}

